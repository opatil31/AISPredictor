{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIS Cohort Builder - UK Biobank RAP\n",
    "\n",
    "This notebook builds the AIS case-control cohort from UK Biobank data.\n",
    "\n",
    "**Inputs:**\n",
    "- `ukb_phenotypes.csv` - Exported phenotype data\n",
    "- `ukb_rel.dat` - KING kinship coefficients\n",
    "\n",
    "**Outputs:**\n",
    "- `cohort.parquet` - Final matched cohort\n",
    "- `matching_info.parquet` - Case-control pairs\n",
    "- `ancestry_pcs.parquet` - Genetic PCs for cohort\n",
    "- `cohort_qc_report.yaml` - QC statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - UPDATE THESE PATHS\n",
    "PROJECT_ID = \"project-J2qppj8JFxzP5QV25fJ8yjQG\"\n",
    "PHENOTYPE_FILE = f\"{PROJECT_ID}:/ukb_phenotypes.csv\"\n",
    "KINSHIP_FILE = f\"{PROJECT_ID}:/Bulk/Genotype Results/Genotype calls/ukb_rel.dat\"\n",
    "OUTPUT_DIR = \"/opt/notebooks/cohort_output\"\n",
    "\n",
    "# Cohort parameters\n",
    "CONTROLS_PER_CASE = 4\n",
    "PCA_OUTLIER_SD = 6.0\n",
    "KINSHIP_THRESHOLD = 0.0884  # 2nd degree relatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"pyarrow\", \"pyyaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yaml\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dxpy\n",
    "\n",
    "# Download phenotype file\n",
    "print(\"Downloading phenotype file...\")\n",
    "local_phenotype = \"/opt/notebooks/ukb_phenotypes.csv\"\n",
    "dxpy.download_dxfile(PHENOTYPE_FILE, local_phenotype)\n",
    "print(f\"Downloaded to {local_phenotype}\")\n",
    "\n",
    "# Download kinship file\n",
    "print(\"Downloading kinship file...\")\n",
    "local_kinship = \"/opt/notebooks/ukb_rel.dat\"\n",
    "dxpy.download_dxfile(KINSHIP_FILE, local_kinship)\n",
    "print(f\"Downloaded to {local_kinship}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load phenotype data\n",
    "print(\"Loading phenotype data...\")\n",
    "pheno_df = pd.read_csv(local_phenotype)\n",
    "print(f\"Loaded {len(pheno_df)} samples\")\n",
    "print(f\"Columns: {list(pheno_df.columns)}\")\n",
    "pheno_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kinship data\n",
    "print(\"Loading kinship data...\")\n",
    "kinship_df = pd.read_csv(local_kinship, sep=\"\\s+\")\n",
    "print(f\"Loaded {len(kinship_df)} related pairs\")\n",
    "print(f\"Columns: {list(kinship_df.columns)}\")\n",
    "kinship_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standardize Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def standardize_phenotype_columns(df):\n    \"\"\"\n    Standardize UK Biobank column names to our expected format.\n    \"\"\"\n    # Print original columns for debugging\n    print(f\"Original columns: {list(df.columns)}\")\n    \n    rename_map = {}\n    \n    for col in df.columns:\n        col_lower = col.lower()\n        \n        # Find eid column\n        if col_lower in ['eid', 'participant.eid', 'participant_eid']:\n            rename_map[col] = 'eid'\n        \n        # Sex (field 31) - check multiple patterns\n        elif any(x in col_lower for x in ['p31', 'sex', 'gender']) or col in ['31', 'p31']:\n            # Also handle participant.p31, p31_i0, etc.\n            if 'sex' not in rename_map.values():  # Only map once\n                rename_map[col] = 'sex'\n        \n        # Age at recruitment (field 21003)\n        elif 'p21003' in col_lower or '21003' in col:\n            rename_map[col] = 'age'\n        \n        # Ethnic background (field 21000)\n        elif 'p21000' in col_lower or '21000' in col:\n            rename_map[col] = 'ethnicity'\n        \n        # Genetic ethnicity (field 22006)\n        elif 'p22006' in col_lower or '22006' in col:\n            rename_map[col] = 'genetic_ethnicity'\n        \n        # Genetic PCs (field 22009)\n        elif 'p22009' in col_lower or '22009' in col:\n            # Extract PC number from various formats: p22009_a1, 22009-0.1, etc.\n            match = re.search(r'[_a.-](\\d+)$', col)\n            if match:\n                pc_num = int(match.group(1))\n                rename_map[col] = f'pc{pc_num}'\n        \n        # ICD-10 diagnoses (field 41270)\n        elif 'p41270' in col_lower or '41270' in col:\n            rename_map[col] = 'diagnoses'\n    \n    # Apply renaming\n    df_renamed = df.rename(columns=rename_map)\n    \n    print(f\"Column rename mapping: {rename_map}\")\n    print(f\"Final columns after rename: {list(df_renamed.columns)}\")\n    \n    # Warn if sex column wasn't found\n    if 'sex' not in df_renamed.columns:\n        print(\"WARNING: 'sex' column not found! Check if p31 was exported.\")\n        print(\"Looking for any column that might be sex...\")\n        for col in df_renamed.columns:\n            print(f\"  - {col}: first values = {df_renamed[col].head(3).tolist()}\")\n    \n    return df_renamed\n\n# Standardize columns\npheno_df = standardize_phenotype_columns(pheno_df)\nprint(f\"\\nStandardized columns: {list(pheno_df.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Case Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIS ICD-10 codes\n",
    "AIS_CODES = [\"M41.1\", \"M41.2\", \"M411\", \"M412\"]  # With and without dots\n",
    "ALL_SCOLIOSIS_CODES = [\"M41\", \"M41.0\", \"M41.1\", \"M41.2\", \"M41.3\", \"M41.4\", \n",
    "                       \"M41.5\", \"M41.8\", \"M41.9\", \"M410\", \"M411\", \"M412\", \n",
    "                       \"M413\", \"M414\", \"M415\", \"M418\", \"M419\"]\n",
    "\n",
    "def check_diagnosis(diag_str, codes):\n",
    "    \"\"\"\n",
    "    Check if any diagnosis matches the given codes.\n",
    "    Handles both string and list-like diagnosis fields.\n",
    "    \"\"\"\n",
    "    if pd.isna(diag_str):\n",
    "        return False\n",
    "    \n",
    "    diag_str = str(diag_str)\n",
    "    \n",
    "    for code in codes:\n",
    "        if code in diag_str:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Identify cases\n",
    "print(\"Identifying AIS cases...\")\n",
    "pheno_df['is_ais_case'] = pheno_df['diagnoses'].apply(\n",
    "    lambda x: check_diagnosis(x, AIS_CODES)\n",
    ")\n",
    "pheno_df['has_any_scoliosis'] = pheno_df['diagnoses'].apply(\n",
    "    lambda x: check_diagnosis(x, ALL_SCOLIOSIS_CODES)\n",
    ")\n",
    "\n",
    "# Assign labels: 1 = case, 0 = potential control, -1 = excluded\n",
    "def assign_label(row):\n",
    "    if row['is_ais_case']:\n",
    "        return 1\n",
    "    elif row['has_any_scoliosis']:\n",
    "        return -1  # Has other scoliosis, exclude from controls\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pheno_df['label'] = pheno_df.apply(assign_label, axis=1)\n",
    "\n",
    "n_cases = (pheno_df['label'] == 1).sum()\n",
    "n_controls = (pheno_df['label'] == 0).sum()\n",
    "n_excluded = (pheno_df['label'] == -1).sum()\n",
    "\n",
    "print(f\"\\nCase identification results:\")\n",
    "print(f\"  AIS cases (M41.1, M41.2): {n_cases}\")\n",
    "print(f\"  Potential controls: {n_controls}\")\n",
    "print(f\"  Excluded (other scoliosis): {n_excluded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ancestry Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# European ancestry codes (field 21000)\n",
    "EUROPEAN_CODES = [1, 1001, 1002, 1003, \"1\", \"1001\", \"1002\", \"1003\",\n",
    "                  \"British\", \"Irish\", \"White\", \"Any other white background\"]\n",
    "\n",
    "# Filter to non-excluded samples\n",
    "df = pheno_df[pheno_df['label'] >= 0].copy()\n",
    "print(f\"Starting with {len(df)} samples (excluding other scoliosis)\")\n",
    "\n",
    "# Step 1: Filter to European ancestry\n",
    "if 'ethnicity' in df.columns:\n",
    "    df['is_european'] = df['ethnicity'].isin(EUROPEAN_CODES)\n",
    "    n_before = len(df)\n",
    "    df = df[df['is_european']].copy()\n",
    "    print(f\"European ancestry filter: {n_before} -> {len(df)} ({n_before - len(df)} removed)\")\n",
    "else:\n",
    "    print(\"Warning: No ethnicity column found, skipping ancestry filter\")\n",
    "\n",
    "# Step 2: PCA outlier removal\n",
    "pc_cols = [col for col in df.columns if col.startswith('pc') and col[2:].isdigit()]\n",
    "print(f\"Found PC columns: {pc_cols}\")\n",
    "\n",
    "if pc_cols:\n",
    "    n_before = len(df)\n",
    "    outlier_mask = pd.Series(False, index=df.index)\n",
    "    \n",
    "    for pc in pc_cols[:4]:  # Use first 4 PCs\n",
    "        pc_values = pd.to_numeric(df[pc], errors='coerce')\n",
    "        mean = pc_values.mean()\n",
    "        std = pc_values.std()\n",
    "        \n",
    "        pc_outliers = (pc_values - mean).abs() > PCA_OUTLIER_SD * std\n",
    "        n_outliers = pc_outliers.sum()\n",
    "        if n_outliers > 0:\n",
    "            print(f\"  {pc}: {n_outliers} outliers (>{PCA_OUTLIER_SD} SD)\")\n",
    "        outlier_mask = outlier_mask | pc_outliers\n",
    "    \n",
    "    df = df[~outlier_mask].copy()\n",
    "    print(f\"PCA outlier removal: {n_before} -> {len(df)} ({n_before - len(df)} removed)\")\n",
    "else:\n",
    "    print(\"Warning: No PC columns found, skipping PCA outlier removal\")\n",
    "\n",
    "print(f\"\\nAfter ancestry QC: {len(df)} samples\")\n",
    "print(f\"  Cases: {(df['label'] == 1).sum()}\")\n",
    "print(f\"  Potential controls: {(df['label'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Relatedness Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Excluding related samples (kinship > {KINSHIP_THRESHOLD})...\")\n\n# Verify we have the eid column\nif 'eid' not in df.columns:\n    print(f\"ERROR: 'eid' column not found in df!\")\n    print(f\"Available columns: {list(df.columns)}\")\n    raise ValueError(\"Missing 'eid' column - check column standardization\")\n\nprint(f\"Using 'eid' column for sample identification\")\nprint(f\"Sample IDs look like: {df['eid'].head(3).tolist()}\")\n\n# Find the kinship column\nkinship_col = None\nfor col in kinship_df.columns:\n    if 'kinship' in col.lower():\n        kinship_col = col\n        break\n\nif kinship_col is None:\n    # Try 'Kinship' with capital K\n    if 'Kinship' in kinship_df.columns:\n        kinship_col = 'Kinship'\n\n# Find ID columns in kinship file\nid_cols = [col for col in kinship_df.columns if 'ID' in col]\nif not id_cols:\n    id_cols = [col for col in kinship_df.columns if 'id' in col.lower()]\n\nprint(f\"Kinship ID columns: {id_cols}\")\nprint(f\"Kinship value column: {kinship_col}\")\n\nif kinship_col and len(id_cols) >= 2:\n    id1_col, id2_col = id_cols[0], id_cols[1]\n    \n    print(f\"Kinship IDs look like: {kinship_df[id1_col].head(3).tolist()}\")\n    \n    # Filter to related pairs above threshold\n    related = kinship_df[kinship_df[kinship_col] > KINSHIP_THRESHOLD]\n    print(f\"Found {len(related)} related pairs above threshold\")\n    \n    # Get samples in our cohort - use 'eid' column explicitly\n    cohort_ids = set(df['eid'].astype(str))\n    label_map = dict(zip(df['eid'].astype(str), df['label']))\n    \n    print(f\"Cohort has {len(cohort_ids)} unique IDs\")\n    \n    # Check for overlap between kinship IDs and cohort IDs\n    kinship_ids = set(related[id1_col].astype(str)) | set(related[id2_col].astype(str))\n    overlap = cohort_ids & kinship_ids\n    print(f\"Kinship file has {len(kinship_ids)} unique IDs in related pairs\")\n    print(f\"Overlap with cohort: {len(overlap)} IDs\")\n    \n    if len(overlap) == 0:\n        print(\"WARNING: No overlap between cohort IDs and kinship IDs!\")\n        print(f\"  Cohort ID examples: {list(cohort_ids)[:3]}\")\n        print(f\"  Kinship ID examples: {list(kinship_ids)[:3]}\")\n    \n    # Identify samples to remove (prefer removing controls over cases)\n    to_remove = set()\n    pairs_in_cohort = 0\n    \n    for _, row in related.iterrows():\n        id1 = str(row[id1_col])\n        id2 = str(row[id2_col])\n        \n        # Both must be in cohort\n        if id1 not in cohort_ids or id2 not in cohort_ids:\n            continue\n        \n        pairs_in_cohort += 1\n        \n        # Skip if one already marked for removal\n        if id1 in to_remove or id2 in to_remove:\n            continue\n        \n        label1 = label_map.get(id1, 0)\n        label2 = label_map.get(id2, 0)\n        \n        # Prefer to remove controls over cases\n        if label1 == 1 and label2 == 0:\n            to_remove.add(id2)\n        elif label1 == 0 and label2 == 1:\n            to_remove.add(id1)\n        else:\n            to_remove.add(id1)  # Remove one arbitrarily\n    \n    print(f\"Related pairs where both are in cohort: {pairs_in_cohort}\")\n    \n    n_before = len(df)\n    df = df[~df['eid'].astype(str).isin(to_remove)].copy()\n    print(f\"Removed {len(to_remove)} related samples: {n_before} -> {len(df)}\")\nelse:\n    print(\"Warning: Could not parse kinship file, skipping relatedness exclusion\")\n\nprint(f\"\\nAfter relatedness exclusion: {len(df)} samples\")\nprint(f\"  Cases: {(df['label'] == 1).sum()}\")\nprint(f\"  Potential controls: {(df['label'] == 0).sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Control Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\nMatching controls to cases ({CONTROLS_PER_CASE}:1 ratio)...\")\n\n# Check what columns are available\nprint(f\"\\nAvailable columns in df: {list(df.columns)}\")\n\n# Prepare matching variables - always start with age, sex\nmatching_vars = ['age', 'sex']\npc_cols_available = [col for col in df.columns if col.startswith('pc') and col[2:].isdigit()]\nmatching_vars.extend(pc_cols_available[:4])  # Add up to 4 PCs\n\nprint(f\"\\nDesired matching variables: {matching_vars}\")\n\n# Check which are actually in the dataframe\npresent_vars = [v for v in matching_vars if v in df.columns]\nmissing_vars = [v for v in matching_vars if v not in df.columns]\n\nif missing_vars:\n    print(f\"WARNING: Missing matching variables: {missing_vars}\")\n    print(\"These will NOT be used for matching!\")\n\n# Convert to numeric and handle missing\nfor var in present_vars:\n    df[var] = pd.to_numeric(df[var], errors='coerce')\n    n_valid = df[var].notna().sum()\n    print(f\"  {var}: {n_valid} valid values ({100*n_valid/len(df):.1f}%)\")\n\n# Split cases and controls\ncases_df = df[df['label'] == 1].copy()\ncontrols_df = df[df['label'] == 0].copy()\n\nprint(f\"\\nCases: {len(cases_df)}, Controls: {len(controls_df)}\")\n\n# Get available matching variables (present and with valid data)\navailable_vars = [v for v in present_vars if df[v].notna().sum() > 0]\nprint(f\"\\nFinal matching variables to use: {available_vars}\")\n\nif 'sex' not in available_vars:\n    print(\"\\n\" + \"=\"*60)\n    print(\"WARNING: SEX IS NOT BEING USED FOR MATCHING!\")\n    print(\"This may result in sex imbalance between cases and controls.\")\n    print(\"Check if p31 was exported from Table Exporter.\")\n    print(\"=\"*60 + \"\\n\")\n\nif not available_vars:\n    raise ValueError(\"No matching variables available!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for matching\n",
    "case_features = cases_df[available_vars].values\n",
    "control_features = controls_df[available_vars].values\n",
    "\n",
    "# Impute missing with mean\n",
    "for i in range(case_features.shape[1]):\n",
    "    all_vals = np.concatenate([case_features[:, i], control_features[:, i]])\n",
    "    mean_val = np.nanmean(all_vals)\n",
    "    case_features[:, i] = np.where(np.isnan(case_features[:, i]), mean_val, case_features[:, i])\n",
    "    control_features[:, i] = np.where(np.isnan(control_features[:, i]), mean_val, control_features[:, i])\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "all_features = np.vstack([case_features, control_features])\n",
    "scaler.fit(all_features)\n",
    "\n",
    "case_features_scaled = scaler.transform(case_features)\n",
    "control_features_scaled = scaler.transform(control_features)\n",
    "\n",
    "print(f\"Feature matrix shapes: cases={case_features_scaled.shape}, controls={control_features_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest neighbor matching\n",
    "print(\"Computing pairwise distances...\")\n",
    "distances = cdist(case_features_scaled, control_features_scaled, metric='euclidean')\n",
    "print(f\"Distance matrix shape: {distances.shape}\")\n",
    "\n",
    "# Perform matching without replacement\n",
    "matched_control_indices = []\n",
    "matching_records = []\n",
    "\n",
    "available_controls = set(range(len(controls_df)))\n",
    "case_ids = cases_df['eid'].values\n",
    "control_ids = controls_df['eid'].values\n",
    "\n",
    "print(f\"Matching {len(case_ids)} cases to up to {CONTROLS_PER_CASE} controls each...\")\n",
    "\n",
    "for i, case_id in enumerate(case_ids):\n",
    "    if not available_controls:\n",
    "        print(f\"Ran out of controls at case {i}\")\n",
    "        break\n",
    "    \n",
    "    # Get distances to available controls\n",
    "    available_list = sorted(list(available_controls))\n",
    "    case_distances = distances[i, available_list]\n",
    "    \n",
    "    # Find nearest controls\n",
    "    n_to_match = min(CONTROLS_PER_CASE, len(available_list))\n",
    "    nearest_indices = np.argsort(case_distances)[:n_to_match]\n",
    "    \n",
    "    for rank, local_idx in enumerate(nearest_indices):\n",
    "        global_idx = available_list[local_idx]\n",
    "        control_id = control_ids[global_idx]\n",
    "        distance = case_distances[local_idx]\n",
    "        \n",
    "        matched_control_indices.append(global_idx)\n",
    "        matching_records.append({\n",
    "            'case_eid': case_id,\n",
    "            'control_eid': control_id,\n",
    "            'match_rank': rank + 1,\n",
    "            'distance': distance,\n",
    "        })\n",
    "        \n",
    "        available_controls.discard(global_idx)\n",
    "\n",
    "# Create results\n",
    "matching_info = pd.DataFrame(matching_records)\n",
    "matched_control_ids = set(matching_info['control_eid'])\n",
    "\n",
    "matched_cases = cases_df.copy()\n",
    "matched_controls = controls_df[controls_df['eid'].isin(matched_control_ids)].copy()\n",
    "matched_cohort = pd.concat([matched_cases, matched_controls], ignore_index=True)\n",
    "\n",
    "n_matched_cases = len(matched_cases)\n",
    "n_matched_controls = len(matched_controls)\n",
    "ratio = n_matched_controls / n_matched_cases if n_matched_cases > 0 else 0\n",
    "\n",
    "print(f\"\\nMatching complete:\")\n",
    "print(f\"  Matched cases: {n_matched_cases}\")\n",
    "print(f\"  Matched controls: {n_matched_controls}\")\n",
    "print(f\"  Ratio: {ratio:.2f}:1\")\n",
    "print(f\"  Mean distance: {matching_info['distance'].mean():.4f}\")\n",
    "print(f\"  Max distance: {matching_info['distance'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Assess Covariate Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCovariate balance assessment:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cases = matched_cohort[matched_cohort['label'] == 1]\n",
    "controls = matched_cohort[matched_cohort['label'] == 0]\n",
    "\n",
    "balance_results = {}\n",
    "\n",
    "for var in available_vars:\n",
    "    case_vals = pd.to_numeric(cases[var], errors='coerce')\n",
    "    control_vals = pd.to_numeric(controls[var], errors='coerce')\n",
    "    \n",
    "    case_mean = case_vals.mean()\n",
    "    control_mean = control_vals.mean()\n",
    "    case_std = case_vals.std()\n",
    "    control_std = control_vals.std()\n",
    "    \n",
    "    # Standardized mean difference\n",
    "    pooled_std = np.sqrt((case_std**2 + control_std**2) / 2)\n",
    "    smd = (case_mean - control_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    balanced = abs(smd) < 0.1\n",
    "    status = \"OK\" if balanced else \"IMBALANCED\"\n",
    "    \n",
    "    balance_results[var] = {\n",
    "        'case_mean': case_mean,\n",
    "        'control_mean': control_mean,\n",
    "        'smd': smd,\n",
    "        'balanced': balanced\n",
    "    }\n",
    "    \n",
    "    print(f\"  {var}: SMD = {smd:.4f} [{status}]\")\n",
    "\n",
    "all_balanced = all(b['balanced'] for b in balance_results.values())\n",
    "print(f\"\\nOverall balance: {'GOOD' if all_balanced else 'NEEDS REVIEW'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final cohort dataframe\n",
    "cohort_cols = ['eid', 'label', 'age', 'sex']\n",
    "cohort_cols.extend([c for c in pc_cols_available[:4] if c in matched_cohort.columns])\n",
    "available_cohort_cols = [c for c in cohort_cols if c in matched_cohort.columns]\n",
    "\n",
    "final_cohort = matched_cohort[available_cohort_cols].copy()\n",
    "final_cohort['is_case'] = final_cohort['label'] == 1\n",
    "\n",
    "# Save cohort\n",
    "cohort_path = f\"{OUTPUT_DIR}/cohort.parquet\"\n",
    "final_cohort.to_parquet(cohort_path, index=False)\n",
    "print(f\"Saved cohort to {cohort_path}\")\n",
    "\n",
    "# Save matching info\n",
    "matching_path = f\"{OUTPUT_DIR}/matching_info.parquet\"\n",
    "matching_info.to_parquet(matching_path, index=False)\n",
    "print(f\"Saved matching info to {matching_path}\")\n",
    "\n",
    "# Save ancestry PCs for matched samples\n",
    "pc_cols_to_save = ['eid'] + [c for c in matched_cohort.columns if c.startswith('pc')]\n",
    "ancestry_pcs = matched_cohort[pc_cols_to_save].copy()\n",
    "pcs_path = f\"{OUTPUT_DIR}/ancestry_pcs.parquet\"\n",
    "ancestry_pcs.to_parquet(pcs_path, index=False)\n",
    "print(f\"Saved ancestry PCs to {pcs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate QC report\n",
    "report = {\n",
    "    'cohort_summary': {\n",
    "        'total_samples': int(len(final_cohort)),\n",
    "        'n_cases': int(n_matched_cases),\n",
    "        'n_controls': int(n_matched_controls),\n",
    "        'control_case_ratio': float(round(ratio, 2)),\n",
    "    },\n",
    "    'matching_summary': {\n",
    "        'mean_distance': float(round(matching_info['distance'].mean(), 4)),\n",
    "        'max_distance': float(round(matching_info['distance'].max(), 4)),\n",
    "        'min_distance': float(round(matching_info['distance'].min(), 4)),\n",
    "    },\n",
    "    'covariate_balance': {\n",
    "        var: {\n",
    "            'standardized_mean_diff': float(round(stats['smd'], 4)),\n",
    "            'balanced': bool(stats['balanced'])\n",
    "        }\n",
    "        for var, stats in balance_results.items()\n",
    "    },\n",
    "    'demographics': {}\n",
    "}\n",
    "\n",
    "if 'age' in final_cohort.columns:\n",
    "    report['demographics']['case_mean_age'] = float(round(cases['age'].mean(), 1))\n",
    "    report['demographics']['control_mean_age'] = float(round(controls['age'].mean(), 1))\n",
    "\n",
    "if 'sex' in final_cohort.columns:\n",
    "    report['demographics']['case_female_pct'] = float(round(100 * (cases['sex'] == 0).mean(), 1))\n",
    "    report['demographics']['control_female_pct'] = float(round(100 * (controls['sex'] == 0).mean(), 1))\n",
    "\n",
    "report_path = f\"{OUTPUT_DIR}/cohort_qc_report.yaml\"\n",
    "with open(report_path, 'w') as f:\n",
    "    yaml.dump(report, f, default_flow_style=False)\n",
    "print(f\"Saved QC report to {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Upload Results to Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload results back to DNAnexus project\n",
    "import dxpy\n",
    "\n",
    "output_files = [\n",
    "    f\"{OUTPUT_DIR}/cohort.parquet\",\n",
    "    f\"{OUTPUT_DIR}/matching_info.parquet\", \n",
    "    f\"{OUTPUT_DIR}/ancestry_pcs.parquet\",\n",
    "    f\"{OUTPUT_DIR}/cohort_qc_report.yaml\"\n",
    "]\n",
    "\n",
    "print(\"Uploading results to project...\")\n",
    "for filepath in output_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    dxpy.upload_local_file(\n",
    "        filepath,\n",
    "        project=PROJECT_ID,\n",
    "        folder=\"/cohort_output\",\n",
    "        parents=True\n",
    "    )\n",
    "    print(f\"  Uploaded {filename}\")\n",
    "\n",
    "print(f\"\\nAll files uploaded to {PROJECT_ID}:/cohort_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COHORT BUILDING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFinal Cohort:\")\n",
    "print(f\"  Total samples: {len(final_cohort)}\")\n",
    "print(f\"  Cases: {n_matched_cases}\")\n",
    "print(f\"  Controls: {n_matched_controls}\")\n",
    "print(f\"  Ratio: {ratio:.2f}:1\")\n",
    "print(f\"\\nOutput files in {PROJECT_ID}:/cohort_output/\")\n",
    "print(f\"  - cohort.parquet\")\n",
    "print(f\"  - matching_info.parquet\")\n",
    "print(f\"  - ancestry_pcs.parquet\")\n",
    "print(f\"  - cohort_qc_report.yaml\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}